# Configuration
# Use UNC paths when running over the network from another machine
input_file: '\\Lenovo\d\DATAPREDICT\DATAPREDICT 2024\Missions\Digora\export_everwin (19).xlsx'

input_file_cleaned_1: '\\Lenovo\d\DATAPREDICT\DATAPREDICT 2024\Missions\Digora\phase1_output\export_phase1_cleaned.csv'
input_file_cleaned_3_all: '\\Lenovo\d\DATAPREDICT\DATAPREDICT 2024\Missions\Digora\phase3_output\phase3_cleaned_all.csv'
input_file_cleaned_3_multi: '\\Lenovo\d\DATAPREDICT\DATAPREDICT 2024\Missions\Digora\phase3_output\phase3_cleaned_multivariate.csv'
input_file_cleaned_3_univ: '\\Lenovo\d\DATAPREDICT\DATAPREDICT 2024\Missions\Digora\phase3_output\phase3_cleaned_univ.csv'

output_dir: '\\Lenovo\d\DATAPREDICT\DATAPREDICT 2024\Missions\Digora\phase4_output'
metrics_dir: '\\Lenovo\d\DATAPREDICT\DATAPREDICT 2024\Missions\Digora\METRICS_DIR'
data_dictionary: '\\Lenovo\d\DATAPREDICT\DATAPREDICT 2024\Missions\Digora\phase1_output\data_dictionary_complet.xlsx'
output_pdf: '\\Lenovo\d\DATAPREDICT\DATAPREDICT 2024\Missions\Digora\phase4_output\phase4_report.pdf'

n_jobs: -1
optimize_params: false
ignore_schema: true
methods: [famd, pca, mca, mfa, umap, pacmap, phate]

# Method specific options (can be left empty)
famd:
  n_components: 12
  # Let fine-tuning choose the component rule (variance, kaiser or elbow)
  n_components_rule:
  variance_threshold: 0.9
  kmeans:
    raw: [ 2, 3, 4, 5, 8, 10, 14 ]
    cleaned_1: [ 2, 3, 6, 8, 12, 14, 15 ]
    cleaned_3_univ: [ 2, 3, 10, 14, 15 ]
    cleaned_3_multi: [ 3, 4, 6, 7, 9, 11, 12, 14 ]
  agglomerative:
    raw: [ 2, 3, 11, 12, 13, 14, 15 ]
    cleaned_1: [ 2, 3, 7, 8, 12, 13, 14, 15 ]
    cleaned_3_univ: [ 2, 3, 4, 11, 12 ]
    cleaned_3_multi: [ 2, 3, 4, 5, 14, 15]
  gaussian:
    raw: [ 2, 3, 7, 12 ]
    cleaned_1: [ 2, 3, 7 ]
    cleaned_3_univ: [ 2, 3, 7, 9 ]
    cleaned_3_multi: [ 2, 3, 4, 7, 8, 12 ]
  spectral:
    raw:
    cleaned_1:
    cleaned_3_univ:
    cleaned_3_multi:
mfa:
  n_components: 10
  weights: null
  segment_col: "Statut commercial"
  # Specify groups only if you want to restrict variables.
  # By default (groups: null), all variables are included automatically.
  groups: null
  kmeans:
    raw: [ 2, 3 ]
    cleaned_1: [ 2, 3 ]
    cleaned_3_univ: [ 2, 3, 11 ]
    cleaned_3_multi: [ 2, 3 ]
  agglomerative:
    raw: [ 2, 3, 4, 5 ]
    cleaned_1: [ 2, 3 ]
    cleaned_3_univ: [ 2, 3 ]
    cleaned_3_multi: [ 2, 3, 5]
  gaussian:
    raw: [2, 3, 4 ]
    cleaned_1: [ 2, 3 ]
    cleaned_3_univ: [ 2, 3, 4 ]
    cleaned_3_multi: [ 2, 3, 4, 8, 11]
  spectral:
    raw:
    cleaned_1:
    cleaned_3_univ:
    cleaned_3_multi:
pca:
  n_components: 3
  svd_solver: randomized
  whiten: true
  kmeans:
    raw: [ 2, 3, 4, 5 ]
    cleaned_1: [ 2, 3 ]
    cleaned_3_univ: [ 2, 3, 4, 5, 7, 9, 15 ]
    cleaned_3_multi: [ 2, 3, 4, 5, 6, 8 ]
  agglomerative:
    raw: [ 2, 3, 4, 5 ]
    cleaned_1: [ 2, 3 ]
    cleaned_3_univ: [ 2, 3, 4, 5 ]
    cleaned_3_multi: [ 2, 5, 8 ]
  gaussian:
    raw: [ 2, 3 ]
    cleaned_1: [ 2, 3, 4, 5 ]
    cleaned_3_univ: [ 2, 3, 5, 6, 8, 10, 13 ]
    cleaned_3_multi: [ 2, 3, 5, 9, 11, 15 ]
  spectral:
    raw:
    cleaned_1:
    cleaned_3_univ:
    cleaned_3_multi:
mca:
  n_components: 15
  normalize: false
  n_iter: 5
  kmeans:
    raw: [ 2, 3, 5, 6, 9, 10 ]
    cleaned_1: [ 2, 3 ]
    cleaned_3_univ: [2, 3, 10, 11, 13, 15 ]
    cleaned_3_multi: [ 2, 3, 4, 6, 10, 13, 14 ]
  agglomerative:
    raw: [ 2, 3, 4, 5, 6 ]
    cleaned_1: [ 2, 3, 15 ]
    cleaned_3_univ: [ 2, 3, 4, 12, 13, 14, 15 ]
    cleaned_3_multi: [ 2, 3, 4, 12, 13, 14, 15]
  gaussian:
    raw: [ 2, 3, 14 ]
    cleaned_1: [ 2, 3, 7, 12 ]
    cleaned_3_univ: [ 2, 3, 7, 11 ]
    cleaned_3_multi: [ 2, 3, 4 ]
  spectral:
    raw:
    cleaned_1:
    cleaned_3_univ:
    cleaned_3_multi:
umap:
  n_neighbors: 30
  min_dist: 0.1
  n_jobs: -1
  # metric will be selected during fine-tuning
  metric: cosine
  n_components: 2
  kmeans:
    raw: [ 3, 5, 6, 11, 12 ]
    cleaned_1: [ 2, 3, 9, 10, 13 ]
    cleaned_3_univ: [ 2, 4, 6, 8 ]
    cleaned_3_multi: [ 3, 4, 6, 8, 11 ]
  agglomerative:
    raw: [ 3, 4, 5, 6, 12 ]
    cleaned_1: [ 2, 3, 6, 13, 15 ]
    cleaned_3_univ: [ 2, 4, 6, 11 ]
    cleaned_3_multi: [ 3, 4, 6, 7, 8, 9 ]
  gaussian:
    raw: [ 2, 3, 5, 9, 10, 12 ]
    cleaned_1: [ 2, 6, 9, 11, 12, 13, 14 ]
    cleaned_3_univ: [ 2, 3, 4, 6, 8, 9 ]
    cleaned_3_multi: [ 3, 4, 6, 8, 9, 10 ]
  spectral:
    raw:
    cleaned_1:
    cleaned_3_univ:
    cleaned_3_multi:
phate:
  knn: 15
  t: 20
  decay: 20
  # n_components selected by fine tuning
  n_components: 3
  kmeans:
    raw: [ 2 ]
    cleaned_1: [ 3, 4, 8, 12, 15 ]
    cleaned_3_univ: [ 2, 3, 5, 6, 8, 9, 11, 12 ]
    cleaned_3_multi: [ 3, 4, 9, 12 ]
  agglomerative:
    raw: [ 2 ]
    cleaned_1: [ 2, 5, 7, 9, 10, 11, 12, 13, 15]
    cleaned_3_univ: [ 3, 5, 6, 9, 10, 11, 13 ]
    cleaned_3_multi: [ 3, 4, 9, 11, 12, 13 ]
  gaussian:
    raw: [ 2 ]
    cleaned_1: [ 3, 5, 6, 11, 14 ]
    cleaned_3_univ: [ 2, 5, 6, 10, 11, 12, 13, 15 ]
    cleaned_3_multi: [ 3, 4, 9, 10, 12, 13, 15]
  spectral:
    raw:
    cleaned_1:
    cleaned_3_univ:
    cleaned_3_multi:
pacmap:
  n_components: 3
  n_neighbors: 5
  MN_ratio: 0.5
  kmeans:
    raw: [ 2, 4, 5, 6, 7, 8, 9, 11, 14]
    cleaned_1: [ 2, 3, 5, 6, 7, 8, 11, 12, 14 ]
    cleaned_3_univ: [ 3, 5, 8, 11, 14 ]
    cleaned_3_multi: [ 3, 5, 7, 10, 12, 13, 14 ]
  agglomerative:
    raw: [ 2, 3, 4, 13, 14, 15 ]
    cleaned_1: [ 2, 4, 9, 10 ]
    cleaned_3_univ: [ 2, 3, 4, 5, 7, 10 ]
    cleaned_3_multi: [ 3, 4, 5, 6, 13, 14 ]
  gaussian:
    raw: [ 2, 4, 7, 8, 9, 10, 11 ]
    cleaned_1: [ 2, 3, 5, 7, 11, 15 ]
    cleaned_3_univ: [ 3, 5, 7, 8, 13 ]
    cleaned_3_multi: [ 2, 3, 5, 6, 7, 10, 14, 15 ]
  spectral:
    raw:
    cleaned_1:
    cleaned_3_univ:
    cleaned_3_multi:

# … vos autres réglages de config …

# -------------------------------------------------------------------
# Section Lead Scoring
# -------------------------------------------------------------------
lead_scoring:
  # 1) Fichier brut d’opportunités (Everwin SX / Digora)
  #    Doit contenir au moins :
  #      - "Date de fin actualisée" (2024-05-21 00:00:00)
  #      - "Statut commercial" (valeurs "Gagné" ou "Perdu")
  #      - les colonnes d’entrée à encoder / normaliser
  input_path: '\\Lenovo\d\DATAPREDICT\DATAPREDICT 2024\Missions\Digora\phase3_output\phase3_cleaned_all.csv'

  # 2) Répertoire où déposer data_cache/ et models/
  #    (créé automatiquement s’il n’existe pas)
  output_dir: '\\Lenovo\d\DATAPREDICT\DATAPREDICT 2024\Missions\Digora\phase4_output'

  # 3) Période pour découper train/validation/test
  #    Tous les enregistrements < test_start → entraînement,
  #    test_start <= date <= test_end → validation,
  #    date > test_end → test final.
  test_start: "2023-01-01"
  test_end:   "2024-01-01"

  # Colonne cible et valeur positive
  target_col: "Statut commercial"        # NE JAMAIS la mettre dans cat_features
  positive_label: "Gagné"                # 1 si == "Gagné", 0 sinon

  # Colonne date utilisée pour le split chronologique
  date_col: "Date de fin actualisée"

  # 4) Liste des variables catégorielles
  cat_features:
    - "Entité opérationnelle"
    #" - "Statut production" # Tant que ce statut n’est pas “Gagné” (ce qui s’exprime en fait via le champ “Statut commercial”), il ne révèle pas la conversion finale. Attention toutefois à ne pas inclure la modalité “Perdu” ici si vous l’utilisez pour construire la cible ; on préfère conserver ce champ pour le feature engineering (ex. fréquence des statuts intermédiaires), mais il ne doit pas être corrélé directement au fait d’être “Gagné” ou “Perdu” comme cible.
    - "Catégorie"
    - "Sous-catégorie"
    - "Type opportunité"

  # 5) Liste des variables numériques
  numeric_features:
    # - "Total recette actualisé" # Leakage
    # - "Total recette réalisé" # Leakage
    # - "Total recette produit" # Leakage
    - "Budget client estimé" # s’il est mis à jour différemment une fois la vente conclue, il peut trahir le statut.
    # Pas de leakage car pas de différence entre avec et sans

  # 6) Paramètres spécifiques XGBoost / CatBoost / LSTM / ARIMA / Prophet
  xgb_params:
   n_estimators: 100
   max_depth: 5

  catboost_params:
   learning_rate: 0.1
   thread_count: 4

  lstm_params:
   batch_size: 32
   epochs: 50
   patience: 5
   intra_threads: 1
   inter_threads: 1
   learning_rate: 0.001

  arima_params:
   n_jobs: 2
   arima_order: [1, 1, 1]   # si vous voulez forcer un ordre

  prophet_params:
   changepoint_prior_scale: 0.05
  prophet_forecast_periods: 12
